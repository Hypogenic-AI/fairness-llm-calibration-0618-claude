\section{Conclusion}
\label{sec:conclusion}

We presented the first controlled study of the AI disclosure penalty in LLM evaluators. Using a within-subjects design with 100 evaluation instances, we showed that \gptfour assigns significantly lower scores ($\Delta = -0.100$, $d = -0.31$, $p = 0.003$) to text labeled as AI-assisted, even when the text is identical to unlabeled controls. This penalty is implicit (the evaluator rarely mentions AI in its reasoning), quality-dependent (strongest for below-average content), and resistant to prompt-based calibration: fairness-aware prompting reduces the penalty by only 3.3\%, while only blind evaluation eliminates it.

These findings have direct implications for the growing ecosystem of AI-mediated evaluation. As regulatory frameworks mandate AI disclosure, organizations must ensure that their evaluation systems do not penalize compliance. Our results suggest that prompt engineering alone is insufficient for this purpose---achieving evaluation fairness with respect to AI disclosure will likely require interventions at the training, architecture, or post-processing level.

\para{Future work.} Three directions are most pressing. First, testing whether the disclosure penalty generalizes across LLM evaluators (Claude, Gemini, open-source models) and evaluation formats (pairwise ranking, open-ended scoring). Second, developing training-based interventions: fine-tuning evaluators on balanced data with and without disclosures, or adapting the bi-level calibration approach of \citet{shen2025exposing} from medical imaging to text evaluation. Third, studying the interaction between AI disclosure and genuine text characteristics---does the penalty change when AI-assisted text is actually distinguishable from human-written text, or is it purely a labeling effect?
