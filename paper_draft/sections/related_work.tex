\section{Related Work}
\label{sec:related_work}

\para{Biases in LLM-as-a-Judge.}
The use of LLMs as automated evaluators has grown rapidly, with GPT-4 serving as the de facto judge in many benchmarks~\citep{zheng2023judging}. However, these evaluators exhibit systematic biases. \citet{wang2023large} documented severe positional bias, showing that GPT-4 reverses its judgment in 46.3\% of cases when response order is swapped. They proposed calibration through evidence-first prompting and position swapping. \citet{ye2024justice} introduced the CALM framework, identifying 12 distinct bias types including positional, verbosity, sentiment, and diversity biases. Their work found that demographic (diversity) bias varies across models, with some LLMs showing robustness rates as low as 0.566. \citet{panickssery2024llm} documented self-preference bias, where LLMs systematically overrate their own outputs. \citet{zheng2024survey} provided a comprehensive survey organizing these biases into a unified taxonomy. Our work identifies a new bias dimension---the AI \disclosurepenalty---that is absent from existing taxonomies.

\para{Demographic bias in automated scoring.}
Fairness in automated evaluation has been studied extensively in educational contexts. \citet{schaller2024fairness} evaluated fairness of automated essay scoring systems (including GPT-4) using psychometric fairness metrics (OSA, OSD, CSD) and found that GPT-4 was generally fair but inconsistent across tasks. More concerning, \citet{yang2025prompt} demonstrated that GPT-4o can infer demographic attributes from essay text with high accuracy ($\sim$82\% for language background) and that scoring errors for non-native English speakers increase when their demographics are correctly identified. \citet{gallegos2024bias} provided a comprehensive survey of bias and fairness in LLMs, establishing theoretical frameworks for operationalizing fairness. Our work extends this line by examining how \emph{explicit} metadata signals---specifically AI disclosure labels---interact with demographic signals in evaluation.

\para{Calibration for fairness.}
Calibration techniques for improving LLM evaluation quality include position swapping~\citep{wang2023large}, evidence-first prompting~\citep{wang2023large}, and verbalized confidence elicitation~\citep{kadavath2023just}. In the multimodal domain, \citet{shen2025exposing} introduced CALIN, a training-free bi-level calibration algorithm that jointly improves calibration and demographic fairness for few-shot in-context learning. CALIN operates on token-level predicted probabilities and reduces both calibration error and demographic performance gaps. However, no calibration approach has been designed to address the AI disclosure dimension. Our work tests whether existing prompt-based calibration strategies---originally developed for positional bias---can also mitigate disclosure bias.

\para{AI disclosure and transparency requirements.}
The EU AI Act~\citep{euaiact2024} and NYC Local Law 144~\citep{nyclaw144} increasingly require transparency about AI involvement in content generation and decision-making. While prior work on AI-generated text has focused on detection~\citep{kim2024prometheus} and watermarking, no work has examined the downstream consequences of honest disclosure on algorithmic evaluation. Our study is the first to measure whether compliance with transparency requirements carries a measurable scoring cost when text is evaluated by LLM judges.
