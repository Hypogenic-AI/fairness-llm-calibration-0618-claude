\begin{abstract}
As AI-assisted writing becomes ubiquitous, regulatory frameworks increasingly require transparent disclosure of AI use. But what happens when the systems evaluating such text are themselves biased against these disclosures? We investigate whether LLM evaluators penalize text that includes explicit AI assistance disclosures, and whether prompt-based calibration strategies can mitigate this bias. Using a controlled within-subjects design with 100 evaluation instances from the Feedback Collection dataset, we measure the effect of prepending ``This response was written with AI assistance'' on scores assigned by GPT-4.1. We find a statistically significant \textbf{AI disclosure penalty} of $-0.100$ points on a 1--5 scale ($p=0.003$, Cohen's $d=-0.31$), with 28\% of samples receiving lower scores. The penalty operates implicitly---the evaluator mentions AI in only 8\% of its reasoning---and concentrates on below-average responses ($-0.217$ for score-2 items, $p=0.012$). We test three calibration strategies: fairness-aware prompting, evidence-first prompting, and blind evaluation. Prompt-based strategies are largely ineffective, reducing the penalty by at most 3.3\%. Only blind evaluation (removing disclosure text) eliminates the bias entirely, reducing the penalty by 83\%. These findings demonstrate that LLM evaluation biases against AI disclosure operate below the level of explicit instruction-following and may require architectural or training-level interventions to address.
\end{abstract}
