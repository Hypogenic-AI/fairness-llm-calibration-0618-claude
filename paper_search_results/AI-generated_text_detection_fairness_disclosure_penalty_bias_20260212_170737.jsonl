{"title": "The Problem with False Positives: AI Detection Unfairly Accuses Scholars of AI Plagiarism", "year": 2024, "authors": "Louie Giray", "url": "https://www.semanticscholar.org/paper/a2a783ce47ed826744eb525859c0e5ebdf294c30", "relevance": 2, "abstract": "ABSTRACT By examining the experiences of scholars from various institutions around the globe, this paper looks into how AI detection tools, which are meant to keep academic integrity intact, may backfire by unfairly accusing scholars of AI plagiarism. This paper shows that false positives disproportionately affect non-native English speakers and scholars with distinctive writing styles. This results in unwarranted accusations that may cause significant harm to their academic careers. Identified also in this paper are several critical issues with current AI detection tools, including algorithmic biases, vulnerabilities to manipulation, and a lack of understanding of context. These shortcomings not only make AI detection tools less effective but also create a climate of anxiety and distrust within academic communities. While scholars are dedicated to maintaining integrity in their writing, AI detection tools may tarnish their reputations by labeling them as cheaters or frauds based on some flawed AI detection results. Therefore, institutions should set clear guidelines and limitations on how AI and AI detection may be used in scholarly work. Also, they should ensure that scholars remain transparent about any AI involvement by including declarations on how it contributed to their writing process. While AI detection tools have value, a healthy skepticism is needed due to the risk of false positives and other limitations. Indeed, AI detection should complement human decision-making, not replace it. This approach could help create a fairer academic environment that balances innovation with the integrity scholars strive for in their work.", "citations": 18}
{"title": "The accuracy-bias trade-offs in AI text detection tools and their impact on fairness in scholarly publication", "year": 2025, "authors": "Ahmad R. Pratama", "url": "https://www.semanticscholar.org/paper/419d0d4b9c407e6c7bca21af84c33e177530ffbe", "relevance": 2, "abstract": "Artificial intelligence (AI) text detection tools are considered a means of preserving the integrity of scholarly publication by identifying whether a text is written by humans or generated by AI. This study evaluates three popular tools (GPTZero, ZeroGPT, and DetectGPT) through two experiments: first, distinguishing human-written abstracts from those generated by ChatGPT o1 and Gemini 2.0 Pro Experimental; second, evaluating AI-assisted abstracts where the original text has been enhanced by these large language models (LLMs) to improve readability. Results reveal notable trade-offs in accuracy and bias, disproportionately affecting non-native speakers and certain disciplines. This study highlights the limitations of detection-focused approaches and advocates a shift toward ethical, responsible, and transparent use of LLMs in scholarly publication.", "citations": 5}
{"title": "A Comparative Analysis of AI Detection Systems", "year": 2025, "authors": "Nikhil Raghav, Abhishek Singh Pawaria", "url": "https://www.semanticscholar.org/paper/c4de4a27a021d5999eb3d7f841c0a9ea86e40b72", "relevance": 2, "abstract": "Abstract\n\nThe increasing sophistication of generative artificial intelligence (AI) and large language models (LLMs) for example,\u00a0 GPT-5 and Gemini is presenting similar challenges in academic integrity and discrimination between AI versus human writing. This research involved a comparative study of five AI content identification systems QuillBot, Writer.com, GPTZero, CrossPlag, and Copyleaks by evaluating the systems with 45 writing samples composed of verified human writing and AI-generated writing. The evaluation of the results was undertaken with generalized statistical measures of sensitivity, specificity, and predictive values. The results suggested, while GPTZero identified with the most balanced accuracy (93%) and CrossPlag identified slightly lower (88%), they did identify students significantly different in specificity levels and additional systems were highly biased toward human writing over AI-generated writing. The findings suggest that as LLMs continue to evolve, modifications to current system may not serve reliably as data in our academic evaluation due to linguistically and semantically based changes. Overall, the research points to the need for hybrid based models, retraining AI content identification system on new data, and the importance of human review, consensus, and engagement in the educational context to ensure fairness and transparency.\n\nKeywords: AI detection, academic integrity, content analysis, machine learning, text classification", "citations": 0}
{"title": "Auditing the Fairness of AI-Detection Tools: A Comparative Study of ESL, Published, and AI-Generated Texts and Their Misclassification Risks", "year": 2025, "authors": "R. Lege", "url": "https://www.semanticscholar.org/paper/b28474f96c2704f53e7358fbbe558e414ddc43ab", "relevance": 2, "abstract": "This study investigated the classification fairness at the threshold level of four commercially available AI detection tools on the Internet: Copyleaks, ZeroGPT, Scribbr, and Quillbot Premium. The research included the submission of three distinct chunks of texts (N=1212) of between 400-5oo words for evaluation. The writing texts came from fully AI-generated examples (N=307), prompted between 2024 and 2025, and published human-written texts (N=302), and ESL graduate student texts (N=303) written before 2021. The texts were analyzed using binary classification thresholds to determine how the three free devices (Copyleaks, ZeroGPT, Scribbr) and the one paid service (QPremium) performed when checking for potentially AI-generated material in each of the writing examples. The study employed a performance metrics to illustrate the issue with threshold application in such devices. The research included the use of the Chi-square test of independence as well as other inferential statistics to assess inter-detector consistency and potential bias patterns. The results indicated that such devices perform well in identifying AI-generated text written artificially; however, significant disparities emerged in the misclassification of human texts. In particular, AI detectors disproportionally flagged ESL writing with false positives. Such findings illustrate the importance of such fairness audits in assessing the linguistic sensitivity in such tools, especially in the educational setting, where misclassification can have academic or reputational consequences.", "citations": 0}
{"title": "GPT detectors are biased against non-native English writers", "year": 2023, "authors": "Weixin Liang, Mert Yuksekgonul, Yining Mao, E. Wu, James Y. Zou", "url": "https://www.semanticscholar.org/paper/e2352eb8e69d7cc0d98f9b13eae33f9c02310ad4", "relevance": 2, "abstract": "", "citations": 423}
{"title": "Perceptions and detection of AI use in manuscript preparation for academic journals", "year": 2023, "authors": "Nir Chemaya, Daniel Martin", "url": "https://www.semanticscholar.org/paper/c9fe3616130adfce3e361ccb6eb23a120f4cd510", "relevance": 1, "abstract": "The rapid advances in Generative AI tools have produced both excitement and worry about how AI will impact academic writing. However, little is known about what norms are emerging around AI use in manuscript preparation or how these norms might be enforced. We address both gaps in the literature by conducting a survey of 271 academics about whether it is necessary to report ChatGPT use in manuscript preparation and by running GPT-modified abstracts from 2,716 published papers through a leading AI detection software to see if these detectors can detect different AI uses in manuscript preparation. We find that most academics do not think that using ChatGPT to fix grammar needs to be reported, but detection software did not always draw this distinction, as abstracts for which GPT was used to fix grammar were often flagged as having a high chance of being written by AI. We also find disagreements among academics on whether more substantial use of ChatGPT to rewrite text needs to be reported, and these differences were related to perceptions of ethics, academic role, and English language background. Finally, we found little difference in their perceptions about reporting ChatGPT and research assistant help, but significant differences in reporting perceptions between these sources of assistance and paid proofreading and other AI assistant tools (Grammarly and Word). Our results suggest that there might be challenges in getting authors to report AI use in manuscript preparation because (i) there is not uniform agreement about what uses of AI should be reported and (ii) journals might have trouble enforcing nuanced reporting requirements using AI detection tools.", "citations": 37}
{"title": "Bias in Content-Generating AI Algorithms: Technical Analysis, Detection, And Mitigation with Python", "year": 2025, "authors": "Augustin Nyembo Mpampi", "url": "https://www.semanticscholar.org/paper/13f9ae65fccd13108919e6ef19cae06f07591d81", "relevance": 1, "abstract": "Generative artificial intelligence models, such as GPT-4, DALL\u00b7E and Stable Diffusion are now essential tools for automated text and image production. However, these models are influenced by algorithmic biases resulting from training data , learning mechanisms and user interactions . These biases, often unconscious, can have significant consequences by reinforcing social stereotypes, excluding certain populations and altering the diversity of generated content.\n\nThis article provides an in-depth technical analysis of the biases present in generative AI. We first explore their origins , highlighting the biases of corpora training , biases introduced by learning algorithms and those induced by users . Then, we present methods for detecting and evaluating biases , using natural language processing (NLP), computer vision and data modeling tools . Experiments in Python illustrate how these biases manifest themselves in text and image models.\n\nFinally, we propose bias mitigation strategies based on several technical approaches: data rebalancing , embedding debiasing , adjustment of cost functions , regulation of outputs And Model auditability . Integrating these techniques helps make AI models fairer and more transparent. The goal is to provide a pragmatic and rigorous approach to designing responsible generative AI models that respect the principles of fairness and diversity.", "citations": 0}
{"title": "Rise of the Machines: The Prevalence and Disclosure of Artificial Intelligence\u2013Generated Text in High-Impact Orthopaedic Journals", "year": 2024, "authors": "Ben D. Pesante, Cyril Mauffrey, J. Parry", "url": "https://www.semanticscholar.org/paper/847290192852f74e40e7f27da444336d49753ed6", "relevance": 1, "abstract": "Introduction: While most orthopaedic journals permit the use of artificial intelligence (AI) in article development, they require that AI not be listed as an author, that authors take full responsibility for its accuracy, and that AI use be disclosed. This study aimed to assess the prevalence and disclosure of AI-generated text in abstracts published in high-impact orthopaedic journals. Methods: Abstracts published from January 1, 2024, to February 19, 2024, in five orthopaedic journals were analyzed: the American Journal of Sports Medicine; the Journal of Arthroplasty; the Journal of Bone and Joint Surgery; the Knee Surgery, Sports, Traumatology, and Arthroscopy (KSSTA) journal; and the BMC Musculoskeletal Disorders (BMC MD) journal. Artificial intelligence detection software was used to evaluate each abstract for AI-generated text. Disclosure of AI use, country of origin, and article type (clinical, preclinical, review, or AI/machine learning) were documented. To evaluate the accuracy of AI detection software, 60 consecutive articles published in the Journal of Bone and Joint Surgery in 2014, before AI writing software was available, were also evaluated. These abstracts were evaluated again after being rewritten with AI writing software. The sensitivity and specificity of the software program for AI-generated text were calculated. Results: A total of 577 abstracts were included in the analysis. AI-generated text was detected in 4.8% of abstracts, ranging from 0% to 12% by journal. Only one (3.6%) of the 28 abstracts with AI-generated text disclosed its use. Abstracts with AI-generated text were more likely to be from the Asian continent (57.1% vs. 28.0%, P = 0.001) and to involve topics of AI or machine learning (21.4% vs. 0.6%, P < 0.0001). The sensitivity and specificity of the AI detection software program were determined to be 91.7% (55/60) and 100% (60/60). Discussion: A small percentage of abstracts published in high-impact orthopaedic journals contained AI-generated text, and most did not report the use of AI despite journal requirements. Level of evidence: Diagnostic Level III.", "citations": 5}
{"title": "Testing of detection tools for AI-generated text", "year": 2023, "authors": "Debora Weber-Wulff, Alla Anohina-Naumeca, Sonja Bjelobaba, T. Folt\u00fdnek, J. Guerrero-Dib, Olumide Popoola, Petr Sigut, Lorna Waddington", "url": "https://www.semanticscholar.org/paper/bd3558bc203b5006d5bcdc214bfcc65430c576d1", "relevance": 1, "abstract": "Recent advances in generative pre-trained transformer large language models have emphasised the potential risks of unfair use of artificial intelligence (AI) generated content in an academic environment and intensified efforts in searching for solutions to detect such content. The paper examines the general functionality of detection tools for AI-generated text and evaluates them based on accuracy and error type analysis. Specifically, the study seeks to answer research questions about whether existing detection tools can reliably differentiate between human-written text and ChatGPT-generated text, and whether machine translation and content obfuscation techniques affect the detection of AI-generated text. The research covers 12 publicly available tools and two commercial systems (Turnitin and PlagiarismCheck) that are widely used in the academic setting. The researchers conclude that the available detection tools are neither accurate nor reliable and have a main bias towards classifying the output as human-written rather than detecting AI-generated text. Furthermore, content obfuscation techniques significantly worsen the performance of tools. The study makes several significant contributions. First, it summarises up-to-date similar scientific and non-scientific efforts in the field. Second, it presents the result of one of the most comprehensive tests conducted so far, based on a rigorous research methodology, an original document set, and a broad coverage of tools. Third, it discusses the implications and drawbacks of using detection tools for AI-generated text in academic settings.", "citations": 354}
{"title": "Evaluating the efficacy of AI content detection tools in differentiating between human and AI-generated text", "year": 2023, "authors": "Ahmed M. Elkhatat, Khaled Elsaid, S. Almeer", "url": "https://www.semanticscholar.org/paper/260b119dce61cdd8affb13782fec7e8f331f9c98", "relevance": 1, "abstract": "The proliferation of artificial intelligence (AI)-generated content, particularly from models like ChatGPT, presents potential challenges to academic integrity and raises concerns about plagiarism. This study investigates the capabilities of various AI content detection tools in discerning human and AI-authored content. Fifteen paragraphs each from ChatGPT Models 3.5 and 4 on the topic of cooling towers in the engineering process and five human-witten control responses were generated for evaluation. AI content detection tools developed by OpenAI, Writer, Copyleaks, GPTZero, and CrossPlag were used to evaluate these paragraphs. Findings reveal that the AI detection tools were more accurate in identifying content generated by GPT 3.5 than GPT 4. However, when applied to human-written control responses, the tools exhibited inconsistencies, producing false positives and uncertain classifications. This study underscores the need for further development and refinement of AI content detection tools as AI-generated content becomes more sophisticated and harder to distinguish from human-written text.", "citations": 271}
{"title": "AI Shaming: The Silent Stigma among Academic Writers and Researchers", "year": 2024, "authors": "L. Giray", "url": "https://www.semanticscholar.org/paper/68360936a5eb661f7d472adfb52081afbb6a8b33", "relevance": 1, "abstract": "", "citations": 17}
{"title": "Prevalence of Artificial Intelligence-Generated Text in Neurosurgical Publications: Implications for Academic Integrity and Ethical Authorship", "year": 2025, "authors": "Daniel Schneider, Akash Mishra, Jacob Gluski, Harshal A Shah, Max Ward, Ethan D. L. Brown, Daniel M. Sciubba, S. Lo", "url": "https://www.semanticscholar.org/paper/878eb7a4005be0587ea997513a0ed6c9bf6627b9", "relevance": 1, "abstract": "Introduction: With the rapid proliferation of artificial intelligence (AI) tools, important questions about their applicability to manuscript preparation have been raised. This study explores the methodological challenges of detecting AI-generated content in neurosurgical publications, using existing detection tools to highlight both the presence of AI content and the fundamental limitations of current detection approaches. Methods: We analyzed 100 randomly selected manuscripts published between 2023 and 2024 in high-impact neurosurgery journals using a two-tiered approach to identify potential AI-generated text. The text was classified as AI-generated if both a robustly optimized bidirectional encoder representations from transformers pretraining approach (RoBERTa)-based AI classification tool yielded a positive classification and the text\u2019s perplexity score was less than 100. Chi-square tests were conducted to assess differences in the prevalence of AI-generated text across various manuscript sections, topics, and types. In an effort to eliminate bias introduced by the more structured nature of abstracts, a subgroup analysis was conducted that excluded abstracts as well. Results: Approximately one in five (20%) manuscripts contained sections flagged as AI-generated. Abstracts and methods sections were disproportionately identified. After excluding abstracts, the association between section type and AI-generated content was no longer statistically significant. Conclusion: Our findings highlight both the increasing integration of AI in manuscript preparation and a critical challenge in academic publishing as AI language models become increasingly sophisticated and traditional detection methods become less reliable. This suggests the need to shift focus from detection to transparency, emphasizing the development of clear disclosure policies and ethical guidelines for AI use in academic writing.", "citations": 3}
{"title": "Detection of GPT-4 Generated Text in Higher Education: Combining Academic Judgement and Software to Identify Generative AI Tool Misuse", "year": 2023, "authors": "Mike Perkins, Jasper Roe, Darius Postma, James McGaughran, Don Hickerson British University Vietnam, Vietnam, James Cook University Singapore, Singapore", "url": "https://www.semanticscholar.org/paper/496dab67b98785b46867173f0d777eaa9a32ca9c", "relevance": 1, "abstract": "This study explores the capability of academic staff assisted by the Turnitin Artificial Intelligence (AI) detection tool to identify the use of AI-generated content in university assessments. 22 different experimental submissions were produced using Open AI\u2019s ChatGPT tool, with prompting techniques used to reduce the likelihood of AI detectors identifying AI-generated content. These submissions were marked by 15 academic staff members alongside genuine student submissions. Although the AI detection tool identified 91% of the experimental submissions as containing AI-generated content, only 54.8% of the content was identified as AI-generated, underscoring the challenges of detecting AI content when advanced prompting techniques are used. When academic staff members marked the experimental submissions, only 54.5% were reported to the academic misconduct process, emphasising the need for greater awareness of how the results of AI detectors may be interpreted. Similar performance in grades was obtained between student submissions and AI-generated content (AI mean grade: 52.3, Student mean grade: 54.4), showing the capabilities of AI tools in producing human-like responses in real-life assessment situations. Recommendations include adjusting the overall strategies for assessing university students in light of the availability of new Generative AI tools. This may include reducing the overall reliance on assessments where AI tools may be used to mimic human writing, or by using AI-inclusive assessments. Comprehensive training must be provided for both academic staff and students so that academic integrity may be preserved.", "citations": 126}
{"title": "Bias Checker AI Web Application: A Framework for Identifying Bias in AI Models", "year": 2025, "authors": "Apurva Gawali", "url": "https://www.semanticscholar.org/paper/9a3428427fcb0e3c708e75a13495ec3fa27e88d1", "relevance": 1, "abstract": "Abstract\u2014\nArtificial Intelligence (AI) models are widely deployed in decision-making systems, but they often exhibit bias due to skewed training data or inherent algorithmic issues. This paper presents a Bias Checker AI Web Application designed to analyze and detect biases in AI-generated outputs. The system uses natural language processing (NLP) and statistical analysis techniques to assess potential biases in text-based predictions. The web-based interface enables [1] real-time bias evaluation, ensuring transparency and fairness in AI systems. The proposed system provides a user-friendly platform for developers and stakeholders to assess their models and mitigate discriminatory outcomes. Additionally, this paper explores the ethical implications of biased AI, potential mitigation techniques, and the importance of transparency in AI-driven decision-making processes.\n\nThe issue of AI bias extends beyond technical flaws, influencing societal and economic structures by reinforcing stereotypes and discriminatory practices. Addressing bias in AI models is crucial for ensuring fairness in automated decision- making. As AI continues to permeate sectors like finance, healthcare, and law enforcement, biased models can perpetuate historical injustices, leading [14] to tangible negative consequences for marginalized groups. This paper emphasizes the role of bias detection tools in fostering trust and accountability in AI applications.\n\nFurthermore, we discuss the significance of incorporating explainability in AI-driven bias detection. The Bias Checker AI Web Application aims to bridge the gap between technical bias analysis and user interpretability, ensuring that results are accessible to both developers and non-technical stakeholders. By integrating intuitive visualization tools and user feedback mechanisms, our system enhances the accessibility of bias detection methodologies.\n\n\nKeywords: Bias detection, AI fairness, Natural Language Processing, Machine Learning, Web Application, Ethical AI, Algorithmic Transparency, AI Ethics.", "citations": 0}
{"title": "Nonhuman \"Authors\" and Implications for the Integrity of Scientific Publication and Medical Knowledge.", "year": 2023, "authors": "A. Flanagin, Kirsten Bibbins-Domingo, M. Berkwits, S. Christiansen", "url": "https://www.semanticscholar.org/paper/4a6c709356777b729258b178c59e8cbf35f0b6c6", "relevance": 1, "abstract": "", "citations": 316}
{"title": "Editors' Statement on the Responsible Use of Generative AI Technologies in Scholarly Journal Publishing.", "year": 2023, "authors": "G. Kaebnick, D. Magnus, Audiey Kao, Mohammad Hosseini, David B. Resnik, Veljko Dubljevi\u0107, Christy Rentmeester, Bert Gordijn, Mark J. Cherry, Karen J Maschke, Lisa M Rasmussen, Laura Haupt, Udo Sch\u00fcklenk, Ruth Chadwick, Debora Diniz", "url": "https://www.semanticscholar.org/paper/570bdefc69462a92fe72fe33d1bfcf79957daff2", "relevance": 1, "abstract": "Generative artificial intelligence (AI) has the potential to transform many aspects of scholarly publishing. Authors, peer reviewers, and editors might use AI in a variety of ways, and those uses might augment their existing work or might instead be intended to replace it. We are editors of bioethics and humanities journals who have been contemplating the implications of this ongoing transformation. We believe that generative AI may pose a threat to the goals that animate our work but could also be valuable for achieving those goals. In the interests of fostering a wider conversation about how generative AI may be used, we have developed a preliminary set of recommendations for its use in scholarly publishing. We hope that the recommendations and rationales set out here will help the scholarly community navigate toward a deeper understanding of the strengths, limits, and challenges of AI for responsible scholarly work.", "citations": 26}
{"title": "Policy framework for the utilization of generative AI", "year": 2024, "authors": "Kunming Cheng, Haiyang Wu", "url": "https://www.semanticscholar.org/paper/21568138fa0c0f284034895a44e1999279d5a6ec", "relevance": 1, "abstract": "", "citations": 7}
{"title": "Navigating academic integrity in biomedical research: the impact of large language models on current practices and future directions.", "year": 2025, "authors": "Anqi Lin, Zuwei Chen, Aimin Jiang, Bufu Tang, Chang Qi, Lingxuan Zhu, Weiming Mou, Wenyi Gan, Dongqiang Zeng, Mingjia Xiao, Guangdi Chu, Shengkun Peng, Hank Z. H. Wong, Lin Zhang, Hengguo Zhang, Xinpei Deng, Quan Cheng, Jian Zhang, Peng Luo", "url": "https://www.semanticscholar.org/paper/a845b1476c4c9ebbb366f9be2c316cd8a232da2b", "relevance": 1, "abstract": "As Large Language Models (LLMs) continue to advance, they have garnered widespread public attention and extensive application across numerous industries and academic disciplines. The proliferation of LLMs has sparked considerable research interest, with studies primarily focusing on their technical characteristics and specific application scenarios. However, systematic research examining the impact of LLMs on academic integrity remains relatively scarce. Academic integrity is of paramount importance in the biomedical field. Therefore, this paper aims to examine both the opportunities and challenges that LLMs present to academic integrity in the biomedical field, and proposes solutions for optimizing the beneficial applications of LLMs. From a positive perspective, LLMs offer substantial benefits to researchers by enhancing research efficiency, improving research quality, and facilitating the generation and dissemination of academic insights. However, they also present numerous challenges, including the potential for promoting academic misconduct, generating content inaccuracies or ambiguous expressions, introducing bias and fairness concerns, compromising peer review mechanisms, facilitating the dissemination of misinformation, and undermining higher education-all of which demand careful attention. To address these issues, we propose solutions and feasible strategies centered on ten core dimensions: establishing policies and regulatory guidelines, enhancing AI literacy and application capabilities, developing and improving relevant technical tools, establishing human-AI collaboration models, reforming peer review procedures and academic evaluation systems, promoting international cooperation and standardization, increasing transparency and strengthening disclosure, reinforcing professional ethics education, and advancing artificial intelligence detection technologies. Overall, while LLMs undoubtedly pose challenges for maintaining academic integrity, their potential for positive impact remains promising. It is anticipated that with technological advancement and improved ethical standards, LLMs will ultimately preserve and strengthen academic integrity.", "citations": 0}
{"title": "FairWrite: Detection and Mitigation of Linguistic Bias in News and Wikipedia Articles", "year": 2025, "authors": "Jahnavi Murali, Alamelu Kannan, N. Sujaudeen", "url": "https://www.semanticscholar.org/paper/cf99d913422fa530c4c9bc306b3a91b5bfcd1967", "relevance": 1, "abstract": "Linguistic bias in news and Wikipedia articles negatively influences public perception and AI systems trained on such data. This study introduces FairWrite, a framework for detecting and mitigating linguistic bias to promote fairer content. Existing methods often lack explainability and computational efficiency. FairWrite addresses these gaps by combining transformer-based bias detection with explainable AI techniques and light-weight models for bias neutralization. The system identifies biased language, highlights key influencing words, and generates neutralized text while preserving meaning. Experiments on news and Wikipedia datasets demonstrate improved accuracy in bias detection and more fluent, contextually appropriate debiasing. By enhancing transparency and efficiency, FairWrite offers a scalable solution for bias-aware content moderation and fairer information dissemination.", "citations": 0}
{"title": "Unveiling Bias and Safety Issues in Generative Models", "year": 2025, "authors": "N. Sebe", "url": "https://www.semanticscholar.org/paper/c5273823e4400e11445a5d6f6a12d25ff6e8a418", "relevance": 1, "abstract": "Recent advances in generative models-specifically Text-to-Image (T2I) systems and vision-language models (VLMs)-have significantly improved the realism, diversity, and accessibility of generated content. However, persistent concerns around fairness, bias, and safety continue to challenge their broader deployment, particularly in high-stakes or public-facing applications. Existing bias detection techniques often rely on a narrow set of predefined categories or are limited to identifying unsafe prompts, overlooking the complex, open-ended nature of real-world bias manifestations. In this presentation, we address these gaps through two key contributions. First, we introduce an open-set bias detection framework tailored for T2I models. Leveraging the interpretive capabilities of large language models (LLMs) and Visual Question Answering (VQA), our method identifies and explains a broad and evolving range of social and representational biases in generated images-without requiring prior enumeration of potential harms. This approach enhances transparency and allows for scalable auditing across diverse use cases. Second, we propose Unsafe Weights Manipulation, a novel, training-free technique for improving the safety of VLMs. By strategically modulating model weights associated with unsafe outputs, we reduce harmful generations without retraining or fine-tuning. To support reliable assessment, we introduce SafeGround, a benchmark and evaluation metric designed to measure safety gains while preserving performance on neutral or safe prompts. Together, these methods provide complementary pathways to enhance both the fairness and robustness of generative AI systems. Our work demonstrates that safety and expressiveness need not be mutually exclusive and offers tools that can be integrated into existing pipelines to promote responsible deployment of generative models.", "citations": 0}
{"title": "Publishers\u2019 and journals\u2019 instructions to authors on use of generative artificial intelligence in academic and scientific publishing: bibliometric analysis", "year": 2024, "authors": "Conner Ganjavi, M. Eppler, Asli Pekcan, Brett Biedermann, Andre Abreu, Gary S. Collins, I. Gill, G. Cacciamani", "url": "https://www.semanticscholar.org/paper/676b1c74535efca92fbb79a26ea66df9ea07e7e7", "relevance": 1, "abstract": "Abstract Objectives To determine the extent and content of academic publishers\u2019 and scientific journals\u2019 guidance for authors on the use of generative artificial intelligence (GAI). Design Cross sectional, bibliometric study. Setting Websites of academic publishers and scientific journals, screened on 19-20 May 2023, with the search updated on 8-9 October 2023. Participants Top 100 largest academic publishers and top 100 highly ranked scientific journals, regardless of subject, language, or country of origin. Publishers were identified by the total number of journals in their portfolio, and journals were identified through the Scimago journal rank using the Hirsch index (H index) as an indicator of journal productivity and impact. Main outcome measures The primary outcomes were the content of GAI guidelines listed on the websites of the top 100 academic publishers and scientific journals, and the consistency of guidance between the publishers and their affiliated journals. Results Among the top 100 largest publishers, 24% provided guidance on the use of GAI, of which 15 (63%) were among the top 25 publishers. Among the top 100 highly ranked journals, 87% provided guidance on GAI. Of the publishers and journals with guidelines, the inclusion of GAI as an author was prohibited in 96% and 98%, respectively. Only one journal (1%) explicitly prohibited the use of GAI in the generation of a manuscript, and two (8%) publishers and 19 (22%) journals indicated that their guidelines exclusively applied to the writing process. When disclosing the use of GAI, 75% of publishers and 43% of journals included specific disclosure criteria. Where to disclose the use of GAI varied, including in the methods or acknowledgments, in the cover letter, or in a new section. Variability was also found in how to access GAI guidelines shared between journals and publishers. GAI guidelines in 12 journals directly conflicted with those developed by the publishers. The guidelines developed by top medical journals were broadly similar to those of academic journals. Conclusions Guidelines by some top publishers and journals on the use of GAI by authors are lacking. Among those that provided guidelines, the allowable uses of GAI and how it should be disclosed varied substantially, with this heterogeneity persisting in some instances among affiliated publishers and journals. Lack of standardization places a burden on authors and could limit the effectiveness of the regulations. As GAI continues to grow in popularity, standardized guidelines to protect the integrity of scientific output are needed.", "citations": 154}
{"title": "Letter to editor: NLP systems such as ChatGPT cannot be listed as an author because these cannot fulfill widely adopted authorship criteria", "year": 2023, "authors": "N. S. Yeo-Teh, B. Tang", "url": "https://www.semanticscholar.org/paper/ce4f78d38e5ae6b3b8361016b2fbb173deb64b8e", "relevance": 1, "abstract": "ABSTRACT This letter to the editor suggests adding a technical point to the new editorial policy expounded by Hosseini et al. on the mandatory disclosure of any use of natural language processing (NLP) systems, or generative AI, in writing scholarly publications. Such AI systems should naturally also be forbidden from being named as authors, because they would not have fulfilled prevailing authorship guidelines (such as the widely adopted ICMJE authorship criteria).", "citations": 59}
